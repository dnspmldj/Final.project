# -*- coding: utf-8 -*-
"""GROUP3_FINAL_PROJECT

Automatically generated by Colaboratory.

import streamlit as st
import time
import sys
from matplotlib import pyplot
import pandas as pd
from PIL import Image
from numpy import mean
from numpy import std
from numpy import argmax
from matplotlib import pyplot as plt
from sklearn.model_selection import KFold
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers import BatchNormalization
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.models import load_model
from keras.utils import load_img
from keras.utils import img_to_array
from keras.utils import to_categorical


@st.cache(allow_output_mutation=True)
def load_model():
  model=tf.keras.models.load_model('saved_fashion.h5')
  return model
  
from PIL import Image,ImageOps
import numpy as np
def import_and_predict(image_data,model):
    size=(64,64)
    image=ImageOps.fit(image_data,size,Image.ANTIALIAS)
    img=np.asarray(image)
    img_reshape=img[np.newaxis,...]
    prediction=model.predict(img_reshape)
    return prediction

st.write("""# Fasion""")

file=st.file_uploader("Choose clothes photo from computer",type=["jpg","png"])

st.image(image,use_column_width=True)

st.success(string)

def summarize_diagnostics(history):
	# plot loss
	pyplot.figure(figsize=(16,10))
	pyplot.subplot(211)
	pyplot.title('Cross Entropy Loss')
	pyplot.plot(history.history['loss'], color='blue', label='train')
	pyplot.plot(history.history['val_loss'], color='orange', label='test')
	# plot accuracy
	pyplot.subplot(212)
	pyplot.title('Classification Accuracy')
	pyplot.plot(history.history['accuracy'], color='blue', label='train')
	pyplot.plot(history.history['val_accuracy'], color='orange', label='test')

# define cnn model
def define_model_3():
	model3 = Sequential()
	model3.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))
	model3.add(BatchNormalization())
	model3.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
	model3.add(BatchNormalization())
	model3.add(MaxPooling2D((2, 2)))
	model3.add(Dropout(0.25))

	model3.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
	model3.add(BatchNormalization())
	model3.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
	model3.add(BatchNormalization())
	model3.add(MaxPooling2D((2, 2)))
	model3.add(Dropout(0.25))

	model3.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
	model3.add(BatchNormalization())
	model3.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
	model3.add(BatchNormalization())
	model3.add(MaxPooling2D((2, 2)))
	model3.add(Dropout(0.25))

	model3.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
	model3.add(BatchNormalization())
	model3.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
	model3.add(BatchNormalization())
	model3.add(MaxPooling2D((2, 2)))
	model3.add(Dropout(0.25))



	model3.add(Flatten())
	model3.add(Dropout(0.25))
	model3.add(Dense(512, activation='relu'))
	model3.add(Dropout(0.25))
	model3.add(Dense(128, activation='relu'))
	model3.add(Dropout(0.25))
	model3.add(Dense(10, activation='softmax'))

	# compile model

	model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
	return model3

early_stop = EarlyStopping(monitor='val_loss', patience=2)

model3= define_model_3()

run_test_harness(model3, 100)

"""### **Finally, a considerably deeper network with three VGG blocks and 512 nodes in the final hidden layer is employed. Each had max norm per block, batch normalization per convolutional layer, and rising dropout rates. Adam was the optimizer, and 100 epochs were utilized. It outperformed the previous two models, as predicted, with 98% accuracy on the dataset and 94% validation. Although there is still some acceptable overfitting, it functioned nicely.**

## **Finalization, saving of model, and testing on new images**
"""

trainX, valX, testX, trainY, valY, testY = prep_pixels(train_X, val_X, test_X, train_Y, val_Y, test_Y)

model3.save('/content/drive/MyDrive/Models/saved_fashion.h5')

model = load_model('/content/drive/MyDrive/Models/saved_fashion.h5')
# evaluate model on test dataset
_, acc = model.evaluate(testX, testY, verbose=0)
print('> %.3f' % (acc * 100.0))

# make a prediction for a new image.
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import img_to_array
from keras.models import load_model

# load and prepare the image
def load_image(filename):
  img = Image.open(filename).resize((224, 224))
  plt.imshow(img)
  plt.show()
  img = load_img(filename, target_size=(28, 28))
  img = img_to_array(img)
  img = img[:,:,0]
  img = img.reshape(1,28, 28, 1)
  img = img.astype('float32')
  img = img / 255.0
  return img

def run_example(filename):
  img = load_image(filename)
  model = load_model('/content/drive/MyDrive/Models/saved_fashion.h5')
  result = np.argmax(model.predict(img), axis=1)
  if result == 0:
    print('Tshirt')
  elif result == 1:
    print('Top')
  elif result == 2:
    print('Pullover')
  elif result == 3:
    print('Dress')
  elif result == 4:
    print('Coat')
  elif result == 5:
    print('Sandal')
  elif result == 6:
    print('Shirt')
  elif result == 7:
    print('Snicker')
  elif result == 8:
    print('Bag')
  else:
    print('Ankle Boot')



